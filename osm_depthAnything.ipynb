{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c048f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv2 cuda devices 1\n",
      "Torch using cuda\n",
      "/home/nick/OSMLoc\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import os\n",
    "\n",
    "print(f\"cv2 cuda devices {cv2.cuda.getCudaEnabledDeviceCount()}\")\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "\n",
    "print(f\"Torch using {DEVICE}\")\n",
    "\n",
    "os.chdir('/home/nick/OSMLoc')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "409bd053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vitl\n",
      "Model created. Checking weight loading...\n",
      "Error checking weights: 'ReLU' object has no attribute 'weight'\n",
      "Weight depth_head.projects.0.weight - mean: -0.000048, std: 0.018037\n",
      "Loaded Model on cuda\n",
      "Total parameters: 336,875,201, Non-zero parameters: 336,601,519\n",
      "Percentage of non-zero weights: 99.92%\n",
      "Input tensor shape: torch.Size([1, 3, 518, 686]), device: cuda:0\n",
      "Input tensor stats - min: -2.5030, max: 3.0917, mean: 0.0715\n",
      "Starting forward pass...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from maploc.models.depth_anything.dpt import DepthAnything\n",
    "\n",
    "# Configuration for DepthAnything model - based on osmloc configuration\n",
    "config = {\n",
    "    'encoder': 'vitl',           # encoder type: 'vits', 'vitb', or 'vitl'\n",
    "    'features': 256,            # feature dimension\n",
    "    'out_dim': 128,             # output dimension\n",
    "    'out_channels': [256, 512, 1024, 1024],  # output channels for each layer\n",
    "    # 'ckpt': 'maploc/models/depth_anything/ckpt/depth_anything_vitl14.pth',  # checkpoint path\n",
    "    'use_bn': False,            # use batch normalization\n",
    "    'use_clstoken': False,      # use class token\n",
    "    'localhub': True,           # load DINOv2 from local hub\n",
    "    'size': [518, 518],        # input size\n",
    "    'val': False               # validation mode\n",
    "}\n",
    "\n",
    "# Create the model with the proper configuration\n",
    "# The DepthAnything class expects the config dict and will unpack it\n",
    "\n",
    "# model = DepthAnything.from_pretrained(\"LiheYoung/depth_anything_{:}14\".format(config.encoder))\n",
    "# Create the model with debugging to check weight loading\n",
    "model = DepthAnything(config)\n",
    "\n",
    "# Check if weights were loaded properly\n",
    "print(f\"Model created. Checking weight loading...\")\n",
    "try:\n",
    "    depth_conv_weight = model.depth_head.scratch.output_conv2[1].weight\n",
    "    print(f\"Depth head output conv weight mean: {depth_conv_weight.mean().item():.6f}\")\n",
    "    print(f\"Depth head output conv weight std: {depth_conv_weight.std().item():.6f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error checking weights: {e}\")\n",
    "    # Try alternative weight checking\n",
    "    try:\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'depth' in name.lower() and param.dim() > 1:\n",
    "                print(f\"Weight {name} - mean: {param.mean().item():.6f}, std: {param.std().item():.6f}\")\n",
    "                break\n",
    "    except Exception as e2:\n",
    "        print(f\"Alternative weight check also failed: {e2}\")\n",
    "\n",
    "model = model.to(DEVICE).eval()\n",
    "\n",
    "print(f\"Loaded Model on {DEVICE}\")\n",
    "\n",
    "# Additional check: verify checkpoint was loaded by checking if weights are non-zero\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "non_zero_params = sum((p != 0).sum().item() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params:,}, Non-zero parameters: {non_zero_params:,}\")\n",
    "print(f\"Percentage of non-zero weights: {100 * non_zero_params / total_params:.2f}%\" if total_params > 0 else \"No parameters found\")\n",
    "\n",
    "\n",
    "# Convert image to tensor and normalize\n",
    "from torchvision.transforms import Compose\n",
    "from maploc.models.depth_anything.util.transform import Resize, NormalizeImage, PrepareForNet\n",
    "\n",
    "transform = Compose([\n",
    "    Resize(\n",
    "        width=518,\n",
    "        height=518,\n",
    "        resize_target=False,\n",
    "        keep_aspect_ratio=True,\n",
    "        ensure_multiple_of=14,\n",
    "        resize_method='lower_bound',\n",
    "        image_interpolation_method=cv2.INTER_CUBIC,\n",
    "    ),\n",
    "    NormalizeImage(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    PrepareForNet(),\n",
    "])\n",
    "\n",
    "image = cv2.cvtColor(cv2.imread('../Data/IMG_20250327_105752435_HDR.jpg'), cv2.COLOR_BGR2RGB) / 255.0\n",
    "image = transform({'image': image})['image']\n",
    "# Debug input tensor\n",
    "image = torch.from_numpy(image).unsqueeze(0).to(DEVICE)\n",
    "print(f\"Input tensor shape: {image.shape}, device: {image.device}\")\n",
    "print(f\"Input tensor stats - min: {image.min().item():.4f}, max: {image.max().item():.4f}, mean: {image.mean().item():.4f}\")\n",
    "\n",
    "# Forward pass with detailed debugging\n",
    "print(\"Starting forward pass...\")\n",
    "with torch.no_grad():\n",
    "    output = model(image)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7e051d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got Depth map with shape: (518, 686)\n",
      "Depth stats - min: 0.0000, max: 0.0000, mean: 0.0000\n",
      "Before normalization - min: 0.0000, max: 0.0000\n",
      "After normalization - min: 0, max: 0\n",
      "Saved depth map to depth.png\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-0.5, 685.5, 517.5, -0.5)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Remove batch dimension and squeeze to get 2D array\n",
    "depth = output[\"simple\"].cpu().detach().numpy().squeeze()  # This removes the batch dimension\n",
    "\n",
    "print(f\"Got Depth map with shape: {depth.shape}\")\n",
    "print(f\"Depth stats - min: {depth.min():.4f}, max: {depth.max():.4f}, mean: {depth.mean():.4f}\")\n",
    "\n",
    "# Debug: check depth values before normalization\n",
    "print(f\"Before normalization - min: {depth.min():.4f}, max: {depth.max():.4f}\")\n",
    "depth_normalized = cv2.normalize(depth, None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "print(f\"After normalization - min: {depth_normalized.min()}, max: {depth_normalized.max()}\")\n",
    "\n",
    "# Save depth map\n",
    "cv2.imwrite('depth.png', depth_normalized)\n",
    "print(f\"Saved depth map to depth.png\")\n",
    "\n",
    "\n",
    "plt.imshow(depth_normalized)\n",
    "plt.title(\"Depth Image\")\n",
    "plt.axis('off')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
